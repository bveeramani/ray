{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0afb29ee",
   "metadata": {},
   "source": [
    "# Fine-tuning a Torch object detection model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e015d8",
   "metadata": {},
   "source": [
    "This tutorial explains how to fine-tune an object detection model using Torch and the\n",
    "[Ray AI Runtime](air) (AIR).\n",
    "\n",
    "You should be familiar with [PyTorch](https://pytorch.org/) before starting the\n",
    "tutorial. If you need a refresher, read PyTorch's\n",
    "[training a classifier](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)\n",
    "tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c69ab62",
   "metadata": {},
   "source": [
    "## Before you begin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f43dfc",
   "metadata": {},
   "source": [
    "* Install the [Ray AI Runtime](air)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba173af",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install 'ray[air]'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6ebf31",
   "metadata": {},
   "source": [
    "* Install `torch`, `torchmetrics`, `torchvision`, and `xmltodict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e67c828",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchmetrics, torchvision xmltodict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961b4127",
   "metadata": {},
   "source": [
    "## Create a `Dataset`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0774c3ad",
   "metadata": {},
   "source": [
    "In this example, you'll train an object detection model on\n",
    "[Pascal VOC](http://host.robots.ox.ac.uk/pascal/VOC/), a canonical object detection\n",
    "dataset. The dataset contains 11,530 images across 20 different classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3978dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_TO_LABEL = {\n",
    "    \"background\": 0,\n",
    "    \"aeroplane\": 1,\n",
    "    \"bicycle\": 2,\n",
    "    \"bird\": 3,\n",
    "    \"boat\": 4,\n",
    "    \"bottle\": 5,\n",
    "    \"bus\": 6,\n",
    "    \"car\": 7,\n",
    "    \"cat\": 8,\n",
    "    \"chair\": 9,\n",
    "    \"cow\": 10,\n",
    "    \"diningtable\": 11,\n",
    "    \"dog\": 12,\n",
    "    \"horse\": 13,\n",
    "    \"motorbike\": 14,\n",
    "    \"person\": 15,\n",
    "    \"pottedplant\": 16,\n",
    "    \"sheep\": 17,\n",
    "    \"sofa\": 18,\n",
    "    \"train\": 19,\n",
    "    \"tvmonitor\": 20,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831a88f7",
   "metadata": {},
   "source": [
    "### Download Pascal VOC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1462fa8c",
   "metadata": {},
   "source": [
    "First, download the 2GB of raw data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207298f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -OJ http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bea47f",
   "metadata": {},
   "source": [
    "Then, untar the raw data to create the `VOCdevkit/VOC2012` folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ca2e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xvf VOCtrainval_11-May-2012.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16b5d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls VOCdevkit/VOC2012"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32ce171",
   "metadata": {},
   "source": [
    "### Define a custom datasource"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d15ef6",
   "metadata": {},
   "source": [
    "Each image has an annotation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06de8da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "image = Image.open(\"VOCDevkit/VOC2012/JPEGImages/2007_000123.jpg\")\n",
    "display(image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f12676d",
   "metadata": {},
   "source": [
    "Each annotation describes the objects in an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3fb7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat VOCdevkit/VOC2012/Annotations/2007_000123.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8192b67",
   "metadata": {},
   "source": [
    "A Datasource is an object that reads data of a particular type. For example, Ray\n",
    "implements a Datasource that reads CSV files.\n",
    "\n",
    "Ray doesn't provide built-in support for Pascal VOC annotations, so you'll need to define a\n",
    "custom datasource. To implement the datasource, extend the built-in `FileBasedDatasource` class\n",
    "and override the `_read_file` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfc0887",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "import xmltodict\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "\n",
    "from ray.data.datasource import FileBasedDatasource\n",
    "from ray.data.extensions import TensorArray\n",
    "\n",
    "\n",
    "class VOCAnnotationDatasource(FileBasedDatasource):\n",
    "    def _read_file(self, f: pa.NativeFile, path: str, **reader_args) -> pd.DataFrame:\n",
    "        text = f.readall().decode(\"utf-8\")\n",
    "        annotation = xmltodict.parse(text)[\"annotation\"]\n",
    "\n",
    "        objects = annotation[\"object\"]\n",
    "        # If there's one object, `objects` is a `dict`; otherwise, it's a `list[dict]`.\n",
    "        if isinstance(objects, dict):\n",
    "            objects = [objects]\n",
    "\n",
    "        boxes: List[Tuple] = []\n",
    "        for obj in objects:\n",
    "            x1 = float(obj[\"bndbox\"][\"xmin\"])\n",
    "            y1 = float(obj[\"bndbox\"][\"ymin\"])\n",
    "            x2 = float(obj[\"bndbox\"][\"xmax\"])\n",
    "            y2 = float(obj[\"bndbox\"][\"ymax\"])\n",
    "            boxes.append((x1, y1, x2, y2))\n",
    "\n",
    "        labels: List[int] = [CLASS_TO_LABEL[obj[\"name\"]] for obj in objects]\n",
    "\n",
    "        filename = annotation[\"filename\"]\n",
    "\n",
    "        return pd.DataFrame(\n",
    "            {\n",
    "                \"boxes\": TensorArray([boxes]),\n",
    "                \"labels\": TensorArray([labels]),\n",
    "                \"filename\": [filename],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def _rows_per_file(self):\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dde7ece",
   "metadata": {},
   "source": [
    "### Read annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0bcf6c",
   "metadata": {},
   "source": [
    "To load the annotations into a `Dataset`, call `ray.data.read_datasource` and pass\n",
    "your custom datasource to the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b685e48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ray\n",
    "\n",
    "\n",
    "root = os.path.expanduser(\"~/Datasets/VOCdevkit/VOC2012\")\n",
    "annotations: ray.data.Dataset = ray.data.read_datasource(\n",
    "    VOCAnnotationDatasource(), paths=os.path.join(root, \"Annotations\")\n",
    ")\n",
    "annotations.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a958b249",
   "metadata": {},
   "source": [
    "### Load images into memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec03f7c",
   "metadata": {},
   "source": [
    "Each row of `annotations` contains the filename of an image. Write a user-defined\n",
    "function to open the images and add them to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c14675",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def read_images(batch: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n",
    "    images: List[np.ndarray] = []\n",
    "    for filename in batch[\"filename\"]:\n",
    "        path = os.path.join(root, \"JPEGImages\", filename)\n",
    "        image = np.array(Image.open(path))\n",
    "        images.append(image)\n",
    "    batch[\"image\"] = np.array(images, dtype=object)\n",
    "    return batch\n",
    "\n",
    "\n",
    "dataset = annotations.map_batches(read_images)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11c5d46",
   "metadata": {},
   "source": [
    "### Split the dataset into train and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73878ece",
   "metadata": {},
   "source": [
    "Once you've created a `Dataset`, split the dataset into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47caefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = dataset.train_test_split(0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbe1299",
   "metadata": {},
   "source": [
    "## Define preprocessing logic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6713478d",
   "metadata": {},
   "source": [
    "A `Preprocessor` is an object that defines preprocessing logic. It's the standard way\n",
    "to preprocess data with Ray.\n",
    "\n",
    "To preprocess the images, create a `TorchVisionPreprocessor` and call `transform` on the\n",
    "dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f286f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "from ray.data.preprocessors import TorchVisionPreprocessor\n",
    "\n",
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "    ]\n",
    ")\n",
    "train_preprocessor = TorchVisionPreprocessor(\n",
    "    columns=[\"image\"], transform=train_transform\n",
    ")\n",
    "train_dataset = train_preprocessor.transform(train_dataset)\n",
    "\n",
    "test_transform = transforms.ToTensor()\n",
    "test_preprocessor = TorchVisionPreprocessor(columns=[\"image\"], transform=test_transform)\n",
    "test_dataset = test_preprocessor.transform(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060ad9e5",
   "metadata": {},
   "source": [
    "## Fine-tune the object detection model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c769858a",
   "metadata": {},
   "source": [
    "### Define the training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e22f0e4",
   "metadata": {},
   "source": [
    "Write a function that trains `fasterrcnn_resnet50_fpn`. Your\n",
    "function should contain standard Torch code with the following changes:\n",
    "1. Wrap your model with `ray.train.torch.prepare_model` instead of `DistributedDataParallel`.\n",
    "2. Distribute data with `session.get_dataset_shard` instead of `DistributedSampler`.\n",
    "3. Iterate over data with `DatasetIterator.iter_batches` instead of `DataLoader`.\n",
    "5. Report metrics and checkpoints with `session.report`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b7fe0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "\n",
    "from ray.air import Checkpoint\n",
    "from ray.air import session\n",
    "\n",
    "\n",
    "def train_one_epoch(*, model, optimizer, batch_size, epoch):\n",
    "    model.train()\n",
    "\n",
    "    lr_scheduler = None\n",
    "    if epoch == 0:\n",
    "        warmup_factor = 1.0 / 1000\n",
    "        lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "            optimizer, start_factor=warmup_factor, total_iters=250\n",
    "        )\n",
    "\n",
    "    device = ray.train.torch.get_device()\n",
    "    train_dataset_shard = session.get_dataset_shard(\"train\")\n",
    "\n",
    "    batches = train_dataset_shard.iter_batches(batch_size=batch_size)\n",
    "    for batch in batches:\n",
    "        inputs = [torch.as_tensor(image).to(device) for image in batch[\"image\"]]\n",
    "        targets = [\n",
    "            {\n",
    "                \"boxes\": torch.as_tensor(boxes).to(device),\n",
    "                \"labels\": torch.as_tensor(labels).to(device),\n",
    "            }\n",
    "            for boxes, labels in zip(batch[\"boxes\"], batch[\"labels\"])\n",
    "        ]\n",
    "        loss_dict = model(inputs, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        session.report(\n",
    "            {\n",
    "                \"losses\": losses.item(),\n",
    "                \"epoch\": epoch,\n",
    "                \"lr\": optimizer.param_groups[0][\"lr\"],\n",
    "                **{key: value.item() for key, value in loss_dict.items()},\n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "def train_loop_per_worker(config):\n",
    "    model = models.detection.fasterrcnn_resnet50_fpn(num_classes=21)\n",
    "    model = ray.train.prepare_model(model)\n",
    "    parameters = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(\n",
    "        parameters,\n",
    "        lr=config[\"lr\"],\n",
    "        momentum=config[\"momentum\"],\n",
    "        weight_decay=config[\"weight_decay\"],\n",
    "    )\n",
    "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "        optimizer, milestones=config[\"lr_steps\"], gamma=config[\"lr_gamma\"]\n",
    "    )\n",
    "\n",
    "    for epoch in range(0, config[\"epochs\"]):\n",
    "        train_one_epoch(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            batch_size=config[\"batch_size\"],\n",
    "            epoch=epoch,\n",
    "        )\n",
    "        lr_scheduler.step()\n",
    "        checkpoint = Checkpoint.from_dict(\n",
    "            {\n",
    "                \"model\": model.module.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "                \"lr_scheduler\": lr_scheduler.state_dict(),\n",
    "                \"config\": config,\n",
    "                \"epoch\": epoch,\n",
    "            }\n",
    "        )\n",
    "        session.report({}, checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ade99c",
   "metadata": {},
   "source": [
    "### Fine-tune the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e599f263",
   "metadata": {},
   "source": [
    "Once you've defined the training loop, create a `TorchTrainer` and pass the training\n",
    "loop to the constructor. Then, call `TorchTrainer.fit` to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04619f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.air.config import ScalingConfig\n",
    "from ray.train.torch import TorchTrainer\n",
    "\n",
    "\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker=train_loop_per_worker,\n",
    "    train_loop_config={\n",
    "        \"batch_size\": 2,\n",
    "        \"lr\": 0.02,\n",
    "        \"epochs\": 26,\n",
    "        \"momentum\": 0.9,\n",
    "        \"weight_decay\": 1e-4,\n",
    "        \"lr_steps\": [16, 22],\n",
    "        \"lr_gamma\": 0.1,\n",
    "    },\n",
    "    scaling_config=ScalingConfig(num_workers=8, use_gpu=True),\n",
    "    datasets={\"train\": train_dataset},\n",
    ")\n",
    "results = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e685131",
   "metadata": {},
   "source": [
    "## Evaluate the model on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3053978",
   "metadata": {},
   "source": [
    "### Define a custom predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35899ce8",
   "metadata": {},
   "source": [
    "`Predictors` perform inference on batches of data.\n",
    "\n",
    "To make `fasterrcnn_resnet50_fpn` outputs compatible with the `Predictor` interface,\n",
    "subclass  `TorchPredictor` and override the `_predict_numpy` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab52b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "from ray.train.torch import TorchPredictor\n",
    "from ray.air.util.tensor_extensions.pandas import _create_possibly_ragged_ndarray\n",
    "\n",
    "\n",
    "class CustomTorchPredictor(TorchPredictor):\n",
    "    def _predict_numpy(\n",
    "        self, data: np.ndarray, dtype: torch.dtype\n",
    "    ) -> Dict[str, np.ndarray]:\n",
    "        inputs = [torch.as_tensor(image) for image in data[\"image\"]]\n",
    "        assert all(image.dim() == 3 for image in inputs)\n",
    "        outputs = self.call_model(inputs)\n",
    "\n",
    "        predictions = collections.defaultdict(list)\n",
    "        for output in outputs:\n",
    "            for key, value in output.items():\n",
    "                predictions[key].append(value.cpu().detach().numpy())\n",
    "\n",
    "        for key, value in predictions.items():\n",
    "            predictions[key] = _create_possibly_ragged_ndarray(value)\n",
    "        predictions = {\"pred_\" + key: value for key, value in predictions.items()}\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cce8cb",
   "metadata": {},
   "source": [
    "### Generate predictions on the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a97030",
   "metadata": {},
   "source": [
    "Create a `BatchPredictor` and pass `CustomTorchPredictor` to the constructor. Then,\n",
    "call `BatchPredictor.predict` to detect objects in the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b00d654",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train.batch_predictor import BatchPredictor\n",
    "\n",
    "\n",
    "model = models.detection.fasterrcnn_resnet50_fpn(num_classes=21)\n",
    "predictor = BatchPredictor(\n",
    "    results.checkpoint, CustomTorchPredictor, model=model, use_gpu=True\n",
    ")\n",
    "\n",
    "predictions = predictor.predict(\n",
    "    test_dataset,\n",
    "    feature_columns=[\"image\"],\n",
    "    keep_columns=[\"boxes\", \"labels\"],\n",
    "    batch_size=4,\n",
    "    max_scoring_workers=1,\n",
    ")\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5518bce4",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0201addb",
   "metadata": {},
   "source": [
    "Once you've created the `predictions` dataset, iterate over the rows of the dataset\n",
    "and compute the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e34e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "\n",
    "\n",
    "metric = MeanAveragePrecision()\n",
    "for row in predictions.iter_rows():\n",
    "    preds = [\n",
    "        {\n",
    "            \"boxes\": torch.as_tensor(row[\"pred_boxes\"]),\n",
    "            \"scores\": torch.as_tensor(row[\"pred_scores\"]),\n",
    "            \"labels\": torch.as_tensor(row[\"pred_labels\"]),\n",
    "        }\n",
    "    ]\n",
    "    target = [\n",
    "        {\n",
    "            \"boxes\": torch.as_tensor(row[\"boxes\"]),\n",
    "            \"labels\": torch.as_tensor(row[\"labels\"]),\n",
    "        }\n",
    "    ]\n",
    "\n",
    "metric.compute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
