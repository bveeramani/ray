{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da5b9b7e",
   "metadata": {},
   "source": [
    "# Fine-tuning a Torch object detection model\n",
    "\n",
    "This tutorial explains how to fine-tune `fasterrcnn_resnet50_fpn` on\n",
    "[Pascal VOC](http://host.robots.ox.ac.uk/pascal/VOC/), a canonical object detection\n",
    "dataset, using the [Ray AI Runtime](air) for parallel data ingest and training.\n",
    "\n",
    "Here's what you'll do:\n",
    "1. Load Pascal VOC into a Dataset\n",
    "2. Fine-tune `fasterrcnn_resnet50_fpn` (the backbone is pre-trained on ImageNet)\n",
    "3. Evaluate the model's accuracy\n",
    "\n",
    "You should be familiar with [PyTorch](https://pytorch.org/) before starting the\n",
    "tutorial. If you need a refresher, read PyTorch's\n",
    "[training a classifier](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)\n",
    "tutorial.\n",
    "\n",
    "## Before you begin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a6d043",
   "metadata": {},
   "source": [
    "* Install the [Ray AI Runtime](air)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3ae999",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install 'ray[air]'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3d4302",
   "metadata": {},
   "source": [
    "* Install `torch`, `torchmetrics`, `torchvision`, and `xmltodict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8251d9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchmetrics>=0.8 torchvision xmltodict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bf13b8",
   "metadata": {},
   "source": [
    "## Create a `Dataset`\n",
    "\n",
    "[Pascal VOC](http://host.robots.ox.ac.uk/pascal/VOC/) contains 11,530 images across 20\n",
    "different classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10df1b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_TO_LABEL = {\n",
    "    \"background\": 0,\n",
    "    \"aeroplane\": 1,\n",
    "    \"bicycle\": 2,\n",
    "    \"bird\": 3,\n",
    "    \"boat\": 4,\n",
    "    \"bottle\": 5,\n",
    "    \"bus\": 6,\n",
    "    \"car\": 7,\n",
    "    \"cat\": 8,\n",
    "    \"chair\": 9,\n",
    "    \"cow\": 10,\n",
    "    \"diningtable\": 11,\n",
    "    \"dog\": 12,\n",
    "    \"horse\": 13,\n",
    "    \"motorbike\": 14,\n",
    "    \"person\": 15,\n",
    "    \"pottedplant\": 16,\n",
    "    \"sheep\": 17,\n",
    "    \"sofa\": 18,\n",
    "    \"train\": 19,\n",
    "    \"tvmonitor\": 20,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab9042f",
   "metadata": {},
   "source": [
    "### Download Pascal VOC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8a58bd",
   "metadata": {},
   "source": [
    "First, download the 2GB of raw data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1987c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -OJ http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255ffbad",
   "metadata": {},
   "source": [
    "Then, untar the raw data to create the `VOCdevkit/VOC2012` folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d937b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xf VOCtrainval_11-May-2012.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5806911",
   "metadata": {},
   "source": [
    "The dataset contain several subdirectories. `JPEGImages` contains raw images, and\n",
    "`Annotations` contains XML annotations. The other subdirectories aren't relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac64785",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls VOCdevkit/VOC2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0e57c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls VOCdevkit/VOC2012/JPEGImages | head -n 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b359799e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "!ls VOCdevkit/VOC2012/Annotations | head -n 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f821e93d",
   "metadata": {},
   "source": [
    "### Define a custom datasource\n",
    "\n",
    "Each annotation describes the objects in an image.\n",
    "\n",
    "For example, view this image of a train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29845a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "image = Image.open(\"VOCdevkit/VOC2012/JPEGImages/2007_000123.jpg\")\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ab2cf1",
   "metadata": {},
   "source": [
    "Then, print the image's annotation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5e074a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat VOCdevkit/VOC2012/Annotations/2007_000123.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686f0885",
   "metadata": {},
   "source": [
    "Notice how there's one object labeled \"train\"\n",
    "\n",
    "```\n",
    "<name>train</name>\n",
    "<pose>Unspecified</pose>\n",
    "<truncated>1</truncated>\n",
    "<difficult>0</difficult>\n",
    "<bndbox>\n",
    "        <xmin>1</xmin>\n",
    "        <ymin>26</ymin>\n",
    "        <xmax>358</xmax>\n",
    "        <ymax>340</ymax>\n",
    "</bndbox>\n",
    "```\n",
    "\n",
    "[Ray Datasets](datasets) lets you read and preprocess data in parallel. Datasets doesn't\n",
    "have built-in support for Pascal VOC annotations, so you'll need to define a custom\n",
    "datasource.\n",
    "\n",
    "A Datasource is an object that reads data of a particular type. For example, Datasets\n",
    "implements a Datasource that reads CSV files. Your datasource will parse labels and\n",
    "bounding boxes from XML files. Later, you'll read the corresponding images.\n",
    "\n",
    "To implement the datasource, extend the built-in `FileBasedDatasource` class\n",
    "and override the `_read_file` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4b8820",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "import xmltodict\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "\n",
    "from ray.data.datasource import FileBasedDatasource\n",
    "from ray.data.extensions import TensorArray\n",
    "\n",
    "\n",
    "class VOCAnnotationDatasource(FileBasedDatasource):\n",
    "    def _read_file(self, f: pa.NativeFile, path: str, **reader_args) -> pd.DataFrame:\n",
    "        text = f.readall().decode(\"utf-8\")\n",
    "        annotation = xmltodict.parse(text)[\"annotation\"]\n",
    "\n",
    "        objects = annotation[\"object\"]\n",
    "        # If there's one object, `objects` is a `dict`; otherwise, it's a `list[dict]`.\n",
    "        if isinstance(objects, dict):\n",
    "            objects = [objects]\n",
    "\n",
    "        boxes: List[Tuple] = []\n",
    "        for obj in objects:\n",
    "            x1 = float(obj[\"bndbox\"][\"xmin\"])\n",
    "            y1 = float(obj[\"bndbox\"][\"ymin\"])\n",
    "            x2 = float(obj[\"bndbox\"][\"xmax\"])\n",
    "            y2 = float(obj[\"bndbox\"][\"ymax\"])\n",
    "            boxes.append((x1, y1, x2, y2))\n",
    "\n",
    "        labels: List[int] = [CLASS_TO_LABEL[obj[\"name\"]] for obj in objects]\n",
    "\n",
    "        filename = annotation[\"filename\"]\n",
    "\n",
    "        return pd.DataFrame(\n",
    "            {\n",
    "                \"boxes\": TensorArray([boxes]),\n",
    "                \"labels\": TensorArray([labels]),\n",
    "                \"filename\": [filename],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def _rows_per_file(self):\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d6ed44",
   "metadata": {},
   "source": [
    "### Read annotations\n",
    "\n",
    "To load the annotations into a `Dataset`, call `ray.data.read_datasource` and pass\n",
    "the custom datasource to the constructor. Ray will read the annotations in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4717e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ray\n",
    "\n",
    "\n",
    "root = os.path.abspath(\"./VOCdevkit/VOC2012\")\n",
    "annotations: ray.data.Dataset = ray.data.read_datasource(\n",
    "    VOCAnnotationDatasource(), paths=os.path.join(root, \"Annotations\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3d0ee6",
   "metadata": {},
   "source": [
    "Look at the first two samples. `VOCAnnotationDatasource` should've correctly parsed\n",
    "labels and bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0039edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff0097f",
   "metadata": {},
   "source": [
    "### Load images into memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87846ae1",
   "metadata": {},
   "source": [
    "Each row of `annotations` contains the filename of an image.\n",
    "\n",
    "Write a user-defined function that loads these images. For each annotation, your function will:\n",
    "1. Open the image associated with the annotation.\n",
    "2. Add the image to a new `\"image\"` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494c71d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def read_images(batch: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n",
    "    images: List[np.ndarray] = []\n",
    "    for filename in batch[\"filename\"]:\n",
    "        path = os.path.join(root, \"JPEGImages\", filename)\n",
    "        image = np.array(Image.open(path))\n",
    "        images.append(image)\n",
    "    batch[\"image\"] = np.array(images, dtype=object)\n",
    "    return batch\n",
    "\n",
    "\n",
    "dataset = annotations.map_batches(read_images)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cdc755",
   "metadata": {},
   "source": [
    "### Split the dataset into train and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfddd49",
   "metadata": {},
   "source": [
    "Once you've created a `Dataset`, split the dataset into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92ee5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = dataset.train_test_split(0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b68209a",
   "metadata": {},
   "source": [
    "## Define preprocessing logic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbea4b4",
   "metadata": {},
   "source": [
    "A `Preprocessor` is an object that defines preprocessing logic. It's the standard way\n",
    "to preprocess data with Ray.\n",
    "\n",
    "Create two preprocessors: one to transpose and scale images (`ToTensor`), and another to\n",
    "randomly augment images every epoch (`RandomHorizontalFlip`). You'll later pass these\n",
    "preprocessors to a `Trainer`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbba448e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "from ray.data.preprocessors import TorchVisionPreprocessor\n",
    "\n",
    "transform = transforms.ToTensor(),\n",
    "preprocessor = TorchVisionPreprocessor(columns=[\"image\"], transform=transform)\n",
    "\n",
    "per_epoch_transform = transforms.RandomHorizontalFlip(p=0.5),\n",
    "per_epoch_preprocessor = TorchVisionPreprocessor(columns=[\"image\"], transform=per_epoch_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c647be8",
   "metadata": {},
   "source": [
    "## Fine-tune the object detection model\n",
    "\n",
    "### Define the training loop\n",
    "\n",
    "Write a function that trains `fasterrcnn_resnet50_fpn`. Your code will look like\n",
    "standard Torch code with a few changes.\n",
    "\n",
    "Here are a few things to point out:\n",
    "1. Distribute the model with `ray.train.torch.prepare_model`. Don't use `DistributedDataParallel`.\n",
    "2. Pass your Dataset to the Trainer. The Trainer automatically shards the data across workers.\n",
    "3. Iterate over data with `DatasetIterator.iter_batches`. Don't use a Torch `DataLoader`.\n",
    "4. Pass preprocessors to the Trainer.\n",
    "\n",
    "In addition, report metrics and checkpoints with `session.report`. `session.report` tracks these metrics in Ray AIR's internal bookkeeping, allowing you to monitor training and analyze training runs after they've finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ec65fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "\n",
    "from ray.air import Checkpoint\n",
    "from ray.air import session\n",
    "\n",
    "\n",
    "def train_one_epoch(*, model, optimizer, batch_size, epoch):\n",
    "    model.train()\n",
    "\n",
    "    lr_scheduler = None\n",
    "    if epoch == 0:\n",
    "        warmup_factor = 1.0 / 1000\n",
    "        lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "            optimizer, start_factor=warmup_factor, total_iters=250\n",
    "        )\n",
    "\n",
    "    device = ray.train.torch.get_device()\n",
    "    train_dataset_shard = session.get_dataset_shard(\"train\")\n",
    "\n",
    "    batches = train_dataset_shard.iter_batches(batch_size=batch_size)\n",
    "    for batch in batches:\n",
    "        inputs = [torch.as_tensor(image).to(device) for image in batch[\"image\"]]\n",
    "        targets = [\n",
    "            {\n",
    "                \"boxes\": torch.as_tensor(boxes).to(device),\n",
    "                \"labels\": torch.as_tensor(labels).to(device),\n",
    "            }\n",
    "            for boxes, labels in zip(batch[\"boxes\"], batch[\"labels\"])\n",
    "        ]\n",
    "        loss_dict = model(inputs, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        session.report(\n",
    "            {\n",
    "                \"losses\": losses.item(),\n",
    "                \"epoch\": epoch,\n",
    "                \"lr\": optimizer.param_groups[0][\"lr\"],\n",
    "                **{key: value.item() for key, value in loss_dict.items()},\n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "def train_loop_per_worker(config):\n",
    "    # By default, `fasterrcnn_resnet50_fpn`'s backbone is pre-trained on ImageNet.\n",
    "    model = models.detection.fasterrcnn_resnet50_fpn(num_classes=21)\n",
    "    model = ray.train.torch.prepare_model(model)\n",
    "    parameters = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(\n",
    "        parameters,\n",
    "        lr=config[\"lr\"],\n",
    "        momentum=config[\"momentum\"],\n",
    "        weight_decay=config[\"weight_decay\"],\n",
    "    )\n",
    "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "        optimizer, milestones=config[\"lr_steps\"], gamma=config[\"lr_gamma\"]\n",
    "    )\n",
    "\n",
    "    for epoch in range(0, config[\"epochs\"]):\n",
    "        train_one_epoch(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            batch_size=config[\"batch_size\"],\n",
    "            epoch=epoch,\n",
    "        )\n",
    "        lr_scheduler.step()\n",
    "        checkpoint = Checkpoint.from_dict(\n",
    "            {\n",
    "                \"model\": model.module.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "                \"lr_scheduler\": lr_scheduler.state_dict(),\n",
    "                \"config\": config,\n",
    "                \"epoch\": epoch,\n",
    "            }\n",
    "        )\n",
    "        session.report({}, checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d68c97c",
   "metadata": {},
   "source": [
    "### Fine-tune the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef58891",
   "metadata": {},
   "source": [
    "Once you've defined the training loop, create a `TorchTrainer` and pass the training\n",
    "loop to the constructor. Then, call `TorchTrainer.fit` to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a59e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.air.config import ScalingConfig\n",
    "from ray.train.torch import TorchTrainer\n",
    "\n",
    "\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker=train_loop_per_worker,\n",
    "    train_loop_config={\n",
    "        \"batch_size\": 2,\n",
    "        \"lr\": 0.02,\n",
    "        \"epochs\": 26,\n",
    "        \"momentum\": 0.9,\n",
    "        \"weight_decay\": 1e-4,\n",
    "        \"lr_steps\": [16, 22],\n",
    "        \"lr_gamma\": 0.1,\n",
    "    },\n",
    "    scaling_config=ScalingConfig(num_workers=8, use_gpu=True),\n",
    "    datasets={\"train\": train_dataset},\n",
    "    dataset_config={\n",
    "        # Don't augment test images. Only apply `per_epoch_preprocessor` to the train\n",
    "        # set.\n",
    "        \"train\": DatasetConfig(\n",
    "            per_epoch_preprocessor=per_epoch_preprocessor,\n",
    "        ),\n",
    "    }\n",
    "    preprocessor=preprocessor,\n",
    ")\n",
    "results = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224a1139",
   "metadata": {},
   "source": [
    "## Evaluate the model on test data\n",
    "\n",
    "Now that you've fine-tuned the model, you'll evaluate it on the test data.\n",
    "\n",
    "### Generate predictions on the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc9bac2",
   "metadata": {},
   "source": [
    "`Predictors` let you perform scalable [batch prediction](batch-prediction) and\n",
    "[online inference](air-serving-guide). To evaluate the model, you'll use\n",
    "`BatchPredictor` to perform inference in a distributed fashion.\n",
    "\n",
    "Create a `BatchPredictor` and pass `TorchDetectionPredictor` to the constructor. Then,\n",
    "call `BatchPredictor.predict` to detect objects in the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3cc662",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train.batch_predictor import BatchPredictor\n",
    "from ray.train.torch import TorchDetectionPredictor\n",
    "\n",
    "\n",
    "model = models.detection.fasterrcnn_resnet50_fpn(num_classes=21)\n",
    "predictor = BatchPredictor.from_checkpoint(results.checkpoint, TorchDetectionPredictor, model=model)\n",
    "\n",
    "predictions = predictor.predict(\n",
    "    test_dataset,\n",
    "    feature_columns=[\"image\"],\n",
    "    keep_columns=[\"boxes\", \"labels\"],\n",
    "    batch_size=4,\n",
    "    num_gpus_per_worker=1,\n",
    ")\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f740d0a",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491d8c12",
   "metadata": {},
   "source": [
    "Once you've created the `predictions` dataset, iterate over the rows of the dataset\n",
    "and compute the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ded656",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "\n",
    "\n",
    "metric = MeanAveragePrecision()\n",
    "for row in predictions.iter_rows():\n",
    "    preds = [\n",
    "        {\n",
    "            \"boxes\": torch.as_tensor(row[\"pred_boxes\"]),\n",
    "            \"scores\": torch.as_tensor(row[\"pred_scores\"]),\n",
    "            \"labels\": torch.as_tensor(row[\"pred_labels\"]),\n",
    "        }\n",
    "    ]\n",
    "    target = [\n",
    "        {\n",
    "            \"boxes\": torch.as_tensor(row[\"boxes\"]),\n",
    "            \"labels\": torch.as_tensor(row[\"labels\"]),\n",
    "        }\n",
    "    ]\n",
    "    metric.update(preds, target)\n",
    "\n",
    "metric.compute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
